{"cells":[{"cell_type":"markdown","id":"5140fb90","metadata":{"id":"5140fb90"},"source":["## Time Series Analysis of US Air Quality by State and County\n","\n","### Part II: Prepare webscraping url zip files for updated data set on city and county, and AQI components:\n","\n","Author: Gem Ruby </br>\n","Date: April 2023\n","\n","Reference weburl: https://aqs.epa.gov/aqsweb/airdata/daily_44201_1981.zip"]},{"cell_type":"code","execution_count":null,"id":"d80c51cc","metadata":{"id":"d80c51cc"},"outputs":[],"source":["#import libariries\n","import pandas as pd\n","import os\n","import zipfile"]},{"cell_type":"code","source":["#connect to goolge colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6ozK9OO_52s","executionInfo":{"status":"ok","timestamp":1680360082672,"user_tz":240,"elapsed":20143,"user":{"displayName":"Gem Ruby","userId":"11932730316029761081"}},"outputId":"671064d9-909b-47dd-a780-8cd44e2b2545"},"id":"v6ozK9OO_52s","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#change directory\n","os.chdir('/content/drive/MyDrive/2022 - BrainStation/AirQuality_Capstone/Data')"],"metadata":{"id":"2pXheKBaAWOX"},"id":"2pXheKBaAWOX","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"19197941","metadata":{"id":"19197941"},"outputs":[],"source":["#Define the code no for each particulate to download the files\n","Particulates = {'44201':'Ozone','42401':'SO2','42101':'CO','42602':'NO2','88101':'PM2.5_FRM','88502':'PM2.5_NON','81102':'PM10'}"]},{"cell_type":"code","execution_count":null,"id":"1e4a5ed4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e4a5ed4","executionInfo":{"status":"ok","timestamp":1680360136630,"user_tz":240,"elapsed":105,"user":{"displayName":"Gem Ruby","userId":"11932730316029761081"}},"outputId":"a560c570-6284-4650-8c44-5d93e0075d2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["42401 SO2\n","42101 CO\n","42602 NO2\n","88101 PM2.5_FRM\n","88502 PM2.5_NON\n","81102 PM10\n"]}],"source":["# Loop through each key-value pair in the list\n","for key in Particulates:\n","     print(key, Particulates[key])"]},{"cell_type":"code","source":["#import all necessary libraries\n","import pandas as pd\n","import requests\n","import zipfile\n","import os"],"metadata":{"id":"35Wust4bBlvu"},"id":"35Wust4bBlvu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define function that will download AQI components\n","def download_and_concat(kv_pairs):\n","    # iterate over each key-value pair in the dictionary\n","    for key, value in kv_pairs.items():\n","        # create a folder with the same name as the key if it doesn't already exist\n","        folder_name = value.lower()\n","        os.makedirs(folder_name, exist_ok=True)\n","        \n","        # create a list to hold the URLs for each year\n","        urls = []\n","        years = list(range(2015, 2022))\n","        \n","        # iterate over each year and create the corresponding URL\n","        for year in years:\n","            url = 'https://aqs.epa.gov/aqsweb/airdata/daily_'+key.lower()+'_'+str(year)+'.zip'\n","            urls.append(url)\n","        \n","        # download the zip files for each year and save them in the folder with the key name\n","        dfs = []\n","        for url in urls:\n","            # get the filename from the URL\n","            filename = url.split('/')[-1]\n","            \n","            # download the zip file from the URL\n","            response = requests.get(url)\n","            \n","            # save the zip file in the folder with the key name\n","            with open(os.path.join(folder_name, filename), 'wb') as f:\n","                f.write(response.content)\n","            \n","            # unzip the file and extract all CSV files\n","            with zipfile.ZipFile(os.path.join(folder_name, filename), 'r') as zip_file:\n","                for member in zip_file.namelist():\n","                    # extract only CSV files\n","                    if member.endswith('.csv'):\n","                        csv_filename = os.path.basename(member)\n","                        # read the CSV file into a Pandas DataFrame\n","                        csv_data = pd.read_csv(zip_file.open(member))\n","                        dfs.append(csv_data)\n","        \n","        # concatenate all DataFrames into a single DataFrame\n","        combined_data = pd.concat(dfs, ignore_index=True)\n","        \n","        # save the combined DataFrame for this key to a CSV file\n","        combined_data.to_csv(os.path.join(folder_name, folder_name+'.csv'), index=False)\n"],"metadata":{"id":"vpGzeDqsIoFm"},"id":"vpGzeDqsIoFm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#downlad All data \n","download_and_concat(Particulates)"],"metadata":{"id":"VfLs28rpJMW8"},"id":"VfLs28rpJMW8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_and_concat2(kv_pairs):\n","    # iterate over each key-value pair in the dictionary\n","    for key, value in kv_pairs.items():\n","        # create a folder with the same name as the key if it doesn't already exist\n","        folder_name = value.lower()\n","        os.makedirs(folder_name, exist_ok=True)\n","        \n","        # create a list to hold the URLs for each year\n","        urls = []\n","        years = list(range(2015, 2022))\n","        \n","        # iterate over each year and create the corresponding URL\n","        for year in years:\n","            url = 'https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_'+key.lower()+'_'+str(year)+'.zip'\n","            urls.append(url)\n","        \n","        # download the zip files for each year and save them in the folder with the key name\n","        dfs = []\n","        for url in urls:\n","            # get the filename from the URL\n","            filename = url.split('/')[-1]\n","            \n","            # download the zip file from the URL\n","            response = requests.get(url)\n","            \n","            # save the zip file in the folder with the key name\n","            with open(os.path.join(folder_name, filename), 'wb') as f:\n","                f.write(response.content)\n","            \n","            # unzip the file and extract all CSV files\n","            with zipfile.ZipFile(os.path.join(folder_name, filename), 'r') as zip_file:\n","                for member in zip_file.namelist():\n","                    # extract only CSV files\n","                    if member.endswith('.csv'):\n","                        csv_filename = os.path.basename(member)\n","                        # read the CSV file into a Pandas DataFrame\n","                        csv_data = pd.read_csv(zip_file.open(member))\n","                        dfs.append(csv_data)\n","        \n","        # concatenate all DataFrames into a single DataFrame\n","        combined_data = pd.concat(dfs, ignore_index=True)\n","        \n","        # save the combined DataFrame for this key to a CSV file\n","        combined_data.to_csv(os.path.join(folder_name, folder_name+'.csv'), index=False)\n"],"metadata":{"id":"s9DhQXJMLgZO"},"id":"s9DhQXJMLgZO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pull Daily AQI by CBSA, County 2015 through 2022 (smaller and updated dataset)\n","\n","Updated = {'cbsa':'CBSA','county':'COUNTY'}"],"metadata":{"id":"VDVbSjREJSfy"},"id":"VDVbSjREJSfy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pull additional dat from 2015 through 2022\n","download_and_concat2(Updated)"],"metadata":{"id":"nxnOiW8SJSXr"},"id":"nxnOiW8SJSXr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pull Meteorological Information ()\n","\n","Meteorological = {'WIND':'Wind','TEMP':'Temperature', 'PRESS':'Barometric_Pressure', 'RH_DP':'RH_and_Dewpoint'}"],"metadata":{"id":"m84XKPCwSaMI"},"id":"m84XKPCwSaMI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#download meteorological information\n","def download_and_concat3(kv_pairs):\n","    # iterate over each key-value pair in the dictionary\n","    for key, value in kv_pairs.items():\n","        # create a folder with the same name as the key if it doesn't already exist\n","        folder_name = value.lower()\n","        os.makedirs(folder_name, exist_ok=True)\n","        \n","        # create a list to hold the URLs for each year\n","        urls = []\n","        years = list(range(2015, 2022))\n","        \n","        # iterate over each year and create the corresponding URL\n","        for year in years:\n","            url = 'https://aqs.epa.gov/aqsweb/airdata/daily_'+key.upper()+'_'+str(year)+'.zip'\n","            urls.append(url)\n","        \n","        # download the zip files for each year and save them in the folder with the key name\n","        dfs = []\n","        for url in urls:\n","            # get the filename from the URL\n","            filename = url.split('/')[-1]\n","            \n","            # download the zip file from the URL\n","            response = requests.get(url)\n","            \n","            # save the zip file in the folder with the key name\n","            with open(os.path.join(folder_name, filename), 'wb') as f:\n","                f.write(response.content)\n","            \n","            # unzip the file and extract all CSV files\n","            with zipfile.ZipFile(os.path.join(folder_name, filename), 'r') as zip_file:\n","                for member in zip_file.namelist():\n","                    # extract only CSV files\n","                    if member.endswith('.csv'):\n","                        csv_filename = os.path.basename(member)\n","                        # read the CSV file into a Pandas DataFrame\n","                        csv_data = pd.read_csv(zip_file.open(member))\n","                        dfs.append(csv_data)\n","        \n","        # concatenate all DataFrames into a single DataFrame\n","        combined_data = pd.concat(dfs, ignore_index=True)\n","        \n","        # save the combined DataFrame for this key to a CSV file\n","        combined_data.to_csv(os.path.join(folder_name, folder_name+'.csv'), index=False)"],"metadata":{"id":"f7sC66lYSWAZ"},"id":"f7sC66lYSWAZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#download and combine\n","download_and_concat3(Meteorological)"],"metadata":{"id":"jEWbDsL-SVub"},"id":"jEWbDsL-SVub","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Next steps: \n","- We willl be cleaning and modeling the data once all the information have been entered into the system. "],"metadata":{"id":"zTmSlaEvKH8-"},"id":"zTmSlaEvKH8-"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}